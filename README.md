# 📊 Understanding Public Inquiry Patterns Using 3-1-1 Dataset (City of Vancouver)

## 📘 Project Description
This project presents a comprehensive descriptive analysis of the 3-1-1 Inquiry Volume dataset sourced from the City of Vancouver's Open Data Portal. The analysis focuses on understanding how public inquiries related to city services are distributed over time, by department, type, and communication channel. The study zeroes in on inquiries managed by the **Real Estate & Facilities Management (REFM)** department to inform staffing, resource allocation, and service delivery planning.

The 3-1-1 system serves as a vital communication bridge between citizens and the municipal government. This analysis enables data-driven decisions that enhance the city's operational efficiency and public service responsiveness.

---

## 🌟 Objective
The primary objective is to uncover key patterns and trends in public inquiries to:
- Determine peak periods of public demand.
- Support resource allocation and workforce planning.
- Improve city responsiveness by forecasting trends.
- Establish a basis for future predictive models and automated decision-making.

---

## 📂 Dataset Overview

**Source**: City of Vancouver Open Data Portal  

**Attributes Included**:
- `Department`: The city division responsible for handling the request.
- `Request Type`: The nature of the inquiry (e.g., maintenance, inspection).
- `Year and Month`: Timestamp when the inquiry was logged.
- `Channel`: Method through which the inquiry was made (e.g., phone, email).
- `Number of Records`: Count of inquiries received in a given period.

📷 **Add Image**:  
`/images/dataset-preview.png`  
**Caption**: _Sample preview of raw 3-1-1 dataset showing major columns and sample data_

---

## 🛠️ Methodology

### 🗂 Week 2 – Data Collection and Preparation
- **Infrastructure Setup**: AWS infrastructure was established using VPC and EC2 to securely manage data flow.
- **S3 Bucket Creation**: An S3 bucket named `cityofvan-raw-spa` was created to store raw data.
- **Folder Structure**: A logical folder structure was implemented in the bucket for easy data retrieval and pipeline integration.
- **Data Upload**: The structured CSV file was uploaded, enabling subsequent steps like profiling and transformation.

📷 **Add Images**:
- `/images/vpc-setup.png` — _Screenshot of VPC configuration in AWS_
- `/images/ec2-instance.png` — _EC2 instance launched under VPC_
- `/images/s3-raw-structure.png` — _S3 bucket folder structure (raw folder)_

---

### 🔬 Week 3 – Data Profiling and Cleaning

#### Data Profiling
- Conducted using **AWS Glue DataBrew**.
- A sample of 20,000 rows was analyzed.
- Key findings:
  - No missing or null values.
  - All fields correctly typed and categorized.
  - 11 outliers in `Number_of_Records` retained to maintain data integrity.

📷 **Add Image**:  
`/images/databrew-profiling.png`  
**Caption**: _Glue DataBrew profiling summary showing field distribution and quality_

#### Data Cleaning
- **Transformations Applied**:
  - Whitespace removed from textual fields.
  - Columns renamed for compatibility and clarity.
  - `Year_Month` reformatted to `yyyy-MM`.
  - Retained valid outliers to reflect real public activity spikes or declines.

- **Outputs**:
  - Cleaned CSV (user-friendly)
  - Cleaned PARQUET (system-friendly for querying)

📷 **Add Images**:
- `/images/databrew-recipe.png` — _Transformation steps in Glue DataBrew_
- `/images/s3-cleaned-csv.png` — _Cleaned dataset (CSV format) in S3_
- `/images/s3-cleaned-parquet.png` — _Cleaned dataset (PARQUET format) in S3_

---

### 📃 Week 4 – Data Cataloging and Analysis

#### Data Cataloging
- Used **AWS Glue Crawler** to register the cleaned dataset into a centralized **Glue Data Catalog**.
- Schema including columns, types, and partitions was automatically generated.
- Enabled discoverability and consistency in querying.

📷 **Add Image**:  
`/images/glue-catalog.png`  
**Caption**: _Data schema auto-generated by Glue Crawler_

#### Visual ETL Pipeline
- Built using **AWS Glue Studio**.
- Key transformations:
  - Split `Year_Month` into separate `Year` and `Month` fields.
  - Renamed columns (`Number_of_Records → num_records`).
  - Applied filters to isolate REFM-specific requests.
  - Converted year/month into integers for sorting.
  - Appended a processing timestamp.

📷 **Add Image**:  
`/images/visual-etl-pipeline.png`  
**Caption**: _Visual ETL flow in AWS Glue Studio_

#### Athena Query Execution
- SQL queries executed using **Amazon Athena**:
  - Calculated average monthly inquiry volume.
  - Identified the highest volume month.
  - Determined total annual requests.
  - Pinpointed periods with minimum and maximum demand.

📷 **Add Image**:  
`/images/athena-query.png`  
**Caption**: _Athena SQL interface and query results_

---

## 📊 Descriptive Analysis Results

- **Total Inquiries by Year**: Revealed the workload handled annually.
- **Average Monthly Volume**: Enabled staffing models based on mean demand.
- **Peak Months**: Highlighted the need for extra support during summer and year-end.
- **Channel Analysis**: Showed phone and email as dominant channels.
- **Request Type and Department**: Validated the focus on REFM maintenance inquiries.

---

## 📄 Insights and Findings

- **High-Demand Months**: Typically occurred in June, July, and December.
- **Department Load**: REFM received considerable volume in maintenance-type requests.
- **Stable Trends**: General inquiry volume was consistent year-to-year, supporting long-term planning.
- **Outliers Valid**: Months with exceptionally low/high requests reflect realistic operational cycles and holidays.

---

## 🚀 Recommendations

- **Dynamic Staffing**: Allocate extra staff during months with high volume.
- **Automation**: Use chatbots or auto-responders to handle frequent inquiries.
- **Predictive Planning**: Implement forecasting tools using historical trends.
- **Channel Optimization**: Streamline services around preferred communication channels.

---

## 🛠️ Tools and Technologies Used

- **Cloud Infrastructure**: AWS S3, EC2, VPC  
- **ETL and Cleaning**: AWS Glue, Glue DataBrew  
- **Querying and Analytics**: AWS Glue Catalog, Amazon Athena  
- **Storage Formats**: CSV (user-readable), PARQUET (machine-readable)

---

## 📦 Deliverables

- ✅ Cleaned and structured dataset in S3 (CSV + PARQUET)  
- ✅ AWS Glue visual ETL pipeline  
- ✅ SQL queries and results using Athena  
- ✅ Data catalog with searchable schema  
- ✅ Documentation and detailed project report

📷 **Add Images**:
- `/images/s3-curated-user.png` — _Final user-friendly output in S3_
- `/images/s3-curated-system.png` — _System-friendly (PARQUET) output in S3_

---

> _This project lays the foundation for smart city initiatives through transparent, data-informed governance._
